---
title: "zBLUP"
author: "Chen Guo-Bo [chenguobo@gmail.com]"
date: "`r Sys.Date()`"
output:
 html_document:
    theme: united
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
print(Sys.time())
set.seed(2020)
```

# Table of contents {.tabset .tabset-fade .tabset-pills}

## 0 Multivariate normal distribution (MVN)
$$f_\mathbf{X}(x_1, ...,x_k)=\frac{exp[-\frac{1}{2}\mathbf{(X-\mu)^T\Sigma^{-1}(X-\mu)}]}{\sqrt{(2\pi)^k|\mathbf{\Sigma|}}}$$
in which $|\mathbf{\Sigma}|$ is the determinant of $\mathbf{\Sigma}$.

### Bivariate normal distribution 
$$f(x,y)=\frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}exp \{-\frac{1}{2(1-\rho^2)}[\frac{(x-\mu_X)^2}{\sigma^2_X}+\frac{(x-\mu_Y)^2}{\sigma^2_Y}-2\frac{\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_X}]\}$$
in which $\mathbf{\mu}=\begin{bmatrix}\mu_X\\\mu_Y\end{bmatrix}$, $\mathbf{\Sigma}=\begin{bmatrix}\sigma^2_X&\rho\sigma_X\sigma_Y\\\rho\sigma_X\sigma_Y&\sigma^2_X\end{bmatrix}$

The conditional distribution for $y|x$ is

$$y|x=\rho\frac{\sigma_X}{\sigma_Y}(x-\mu_X)+\mu_Y$$, which is best linear unbiased prediction. 

and the conditional sampling variance is $var(y|x)=(1-\rho^2)\sigma^2_Y$.

### Conditional multivariate 

$$\mathbf{X \sim MVN(\mu, \Sigma)}$$
If we partition it into to components
$$\mathbf{X} \sim MVN(\begin{bmatrix} \mathbf{\mu_{X_1}}\\ \mathbf{\mu_{X_2}} \end{bmatrix},
\begin{bmatrix} \mathbf{\Sigma_{X_1X_1}} & \mathbf{\Sigma_{X_1X_2}} \\ \mathbf{\Sigma_{X_2X_1}} & \mathbf{\Sigma_{X_2X_2}}\end{bmatrix})$$

Its conditional distribution is 
$$\mathbf{X_{2|{X_1}}\sim MVN} (\mathbf{\mu_{X_2}}-\mathbf{\Sigma}_{21}\mathbf{\Sigma}_{22}^{-1}(X_1-\mu_{X_1}), \mathbf{\Sigma_{X_2X_2}}-\mathbf{\Sigma_{21}\mathbf{\Sigma_{11}^{-1}}\Sigma_{12}})$$

### Example (from Walsh example C8.6)

Consider the regression of the phenotypic value of an offspring ($z_o$) on that of its parents ($z_s$ and $z_d$ for sire and dam, respectively). Assume that the joint distribution of zo, zs, and Zd is multivariate normal. For the simplest case of noninbred and unrelated parents, no epistasis or genotype-environment correlation, the covariance matrix can be obtained from the theory of correlation between relatives (Chapter 7), giving the joint distribution as

$$
\begin{bmatrix}
z_o\\
z_s\\
z_d\\
\end{bmatrix}
\sim MVN (
\begin{bmatrix}
\mu_z\\
\mu_s\\
\mu_d\\
\end{bmatrix}
,
\sigma^2_z
\begin{bmatrix}
1 & \frac{h^2}{2} & \frac{h^2}{2}\\
 \frac{h^2}{2} & 1 & 0\\
\frac{h^2}{2} & 0 & 1\\
\end{bmatrix}
)
$$
Let $X_1=z_0$ and $X_2=(z_s, z_d)^T$, giving $\mathbf{V_{X_1, X_1}}=\sigma^2_z$, $\mathbf{V_{X_1, X_2}}=\mathbf{V_{X_2, X_1}^T}=\sigma^2_z(\frac{h^2}{2}\frac{h^2}{2})$ and $\mathbf{V_{X_2, X_2}}$, and $\mathbf{V_{X_2, X_2}}=\mathbf{I}_{2\times2}$. Upon conditional distribution, it is 

$$
\begin{align}
z_o&=\mu_0+\sigma^2_z\frac{h^2}{2}
\begin{bmatrix}1&1\end{bmatrix}
(\sigma^2_z)^{-1}\begin{bmatrix}
z_s-\mu_s\\
z_d-\mu_d
\end{bmatrix}
+e\\
&=\mu_o+\frac{h^2}{2}(z_s-\mu_s)+\frac{h^2}{2}(z_d-\mu_d)+e
\end{align}
$$
The residual is distributed as 
$$
\begin{align}
\sigma^2_e&=\sigma^2_z-\frac{h^2\sigma^2_z}{2}
\begin{bmatrix}1&1\end{bmatrix}
(\sigma^2_z)^{-1}
\begin{bmatrix}
1&0\\
0&1
\end{bmatrix}
\frac{h^2\sigma^2_z}{2}
\begin{bmatrix}
1\\
1
\end{bmatrix}\\
&=\sigma^2_z(1-\frac{h^4}{2})
\end{align}
$$


Further reading

- Lynch & Walsh

- [Multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions)

- [Risk Prediction of Complex Diseases from Family History and Known Susceptibility Loci, with Applications for Cancer Screening, $AJHG,2011,88:648-65$]

## 1 LMM (BLUE & BLUP)
$$\mathbf{y}=\mathbf{\color{red}{X\beta}+\color{blue}{\sum_{k=1}^KZ_ku_k}+\color{green}{e}}$$

Design matrices (or incident matrices):

$\color{red}{\mathbf{X}}$ ($n\times p$) for fixed effects vector $\color{red}{\mathbf{\beta}}$

$\mathbf{Z_k}$ ($n\times q_k$) for random effects vector $\mathbf{u_k}$, and $u_k \sim N(0, \mathbf{G_k})$. For example, if $u_k$ is sampled from iid with variance $\sigma^2_k$, then $u_k \sim N(0, \mathbf{I}\sigma^2_k)$.

$\color{green}{\mathbf{e}}$ is random effects. Often, for unrelated samples, $e \sim N(0, \mathbf{I}\sigma^2_e)$.

Then $\mathbf{y}$ follows the distribution $$MVN(\color{red}{\mathbf{X\beta}}, V=\color{blue}{\sum_{k=1}^KG_k\sigma^2_k+R\sigma^2_e})$$

### Estimator
Using general least squares estimation

$$\mathbf{\color{red}{\hat{\beta}}=(X^TV^{-1}X)^{-1}X^TV^{-1}y}$$
Of note, when $V=I$, it becomes ordinary LSE that $\mathbf{\hat{\beta}=(X^TX)^{-1}X^Ty}$.

The random effect is estimated, known as $\color{cyan}{\boxed{BLUP}}$, can be written as
$$\mathbf{\color{blue}{\hat{u}_k}=G_kZ^T_kV^{-1}(y-X\hat{\beta})}$$


Given $\mathbf{\hat{\beta}}$ and $\hat{\mathbf{u}}$ we have
$$
\mathbf{E}=\left \{
\left [ \begin{array}{l}
\mathbf{\beta}-\mathbf{\hat{\beta}}\\
\mathbf{\mu}-\mathbf{\hat{\mu}}
\end{array}
\right ]
\left [ \begin{array}{l}
\mathbf{\beta}-\mathbf{\hat{\beta}}\\
\mathbf{\mu}-\mathbf{\hat{\mu}}
\end{array}
\right ]^T
\right \}=
\begin{bmatrix}
\mathbf{X^TR^{-1}X} & \mathbf{X^TR^{-1}Z}\\
\mathbf{Z^TR^{-1}X} & \mathbf{X^TR^{-1}Z}+\mathbf{G}^{-1}\\
\end{bmatrix}^{-1}\sigma^2_e
$$

## 2 Mixed model equation (MME)
MME is proposed by Henderson. Here we follow the summary by Robinson (Statistical Science, 1991, 6:15-32),

The joint density of $y$ and $u$ is
$$
(2\pi\sigma^2_e)^{-\frac{1}{2}n-\frac{1}{2}q}
\lgroup det
\begin{bmatrix}
\mathbf{G_1} & 0 & 0\\
0 & \mathbf{G_2} & 0 \\
0 & 0 & \mathbf{R}\\
\end{bmatrix}
\rgroup
^{-1}
\cdot
exp
\left \{
-\frac{1}{2\sigma^2_e}
\begin{bmatrix}
\mathbf{u_1}\\
\mathbf{u_2}\\
\mathbf{y-X\beta-Z_1u_1-Z_2u_2}
\end{bmatrix}^T
\begin{bmatrix}
\mathbf{G_1} & 0 &0\\
0 & \mathbf{G_2} & 0\\
0 & 0 &\mathbf{R} \\
\end{bmatrix}^{-1}
\begin{bmatrix}
\mathbf{u_1}\\
\mathbf{u_2}\\
\mathbf{y-X\beta-Z_1u_1-Z_2u_2}
\end{bmatrix}
\right \}
$$
We can define

$$\mathbf{\mathcal{Q}}=\mathbf{u_1^TG_1^{-1}u_1}+\mathbf{u_2^TG_2^{-1}u_2}+\mathbf{\tilde{y}R^{-1}\tilde{y}}$$

in which $\mathbf{\tilde{y}}=\mathbf{y-X\beta-Z_1u_1-Z_2u_2}$,

$$
\begin{bmatrix}
\mathbf{\frac{\partial\mathcal{Q}}{\partial\beta}}=-2\mathbf{X\tilde{y}+2X^T\beta X+2Z_1^Tu_1Z_1+2Z_2^Tu_2Z_2}=0\\
\mathbf{\frac{\partial\mathcal{Q}}{\partial u_1}}=2\mathbf{G_1^{-1}u_1-2yZ_1+2X\beta Z_1+2Z_1^Tu_1Z_1+2Z_1^Tu_2Z_2}=0\\
\mathbf{\frac{\partial\mathcal{Q}}{\partial u_2}}=2\mathbf{G_2^{-1}u_2-2yZ_2+2X\beta Z_2+2Z_1^Tu_1Z_2+2Z_2^Tu_2Z_2}=0
\end{bmatrix}
$$

It yields MME equation below

$$
\begin{bmatrix}
\mathbf{X^TR^{-1}X} & \mathbf{X^TR^{-1}Z_1} & \mathbf{X^TR^{-1}Z_2} \\
\mathbf{Z_1^TR^{-1}X} & \mathbf{Z_1^TR^{-1}Z_1}+\mathbf{G_1^{-1}} & \mathbf{Z_1^TR^{-1}Z_2}\\
\mathbf{Z_2^TR^{-1}X} & \mathbf{Z_2^TR^{-1}Z_1} & \mathbf{Z_2^TR^{-1}Z_2}+\mathbf{G_2^{-1}}
\end{bmatrix}
\begin{bmatrix}
\mathbf{\hat{\beta}}\\
\mathbf{\hat{u}_1}\\
\mathbf{\hat{u}_2}
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{X^TR^{-1}y}\\
\mathbf{Z_1^TR^{-1}y}\\
\mathbf{Z_2^TR^{-1}y}\\
\end{bmatrix}
$$
When $R=I$, it can be written as

$$
\begin{bmatrix}
\mathbf{X^TX} & \mathbf{X^TZ_1} & \mathbf{X^TZ_2} \\
\mathbf{Z_1^TX} & \mathbf{Z_1^TZ_1}+\lambda_1\mathbf{G_1^{-1}} & \mathbf{Z_1^TZ_2}\\
\mathbf{Z_2^TR^{-1}X} & \mathbf{Z_2^TR^{-1}Z_1} & \mathbf{Z_2^TR^{-1}Z_2}+\lambda_2\mathbf{G_2^{-1}}
\end{bmatrix}
\begin{bmatrix}
\mathbf{\hat{\beta}}\\
\mathbf{\hat{u}_1}\\
\mathbf{\hat{u}_2}
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{X^Ty}\\
\mathbf{Z_1^Ty}\\
\mathbf{Z_2^Ty}\\
\end{bmatrix}
$$
in which $\lambda_1=\sigma^2_e/\sigma^2_1$ and $\lambda_2=\sigma^2_e/\sigma^2_2$.

## Walsh'sCh26 example 1

$$\mathbf{y}=
\begin{bmatrix}
y_{\color{blue}{1},\color{red}{1},1}\\
y_{\color{blue}{1},\color{red}{2},1}\\
y_{\color{blue}{2},\color{red}{1},1}\\
y_{\color{blue}{2},\color{red}{1},2}\\
y_{\color{blue}{3},\color{red}{1},1}\\
y_{\color{blue}{3},\color{red}{2},1}\\
\end{bmatrix}
=
\begin{bmatrix}
9\\
12\\
11\\
6\\
7\\
14
\end{bmatrix}
$$

|$y_{\color{blue}{S,}\color{red}{E,}C}$ | $\color{blue}{Sire}$ (Random) | $\color{red}{Environment}$ (Fixed) | Count | Value |
|:------:|:------:|:------:|:----:|:----:|
| $y_{\color{blue}{1},\color{red}{1},1}$ | $\color{blue}{1}$ | $\color{red}{1}$ | $1$ | 9 |
| $y_{\color{blue}{1},\color{red}{2},1}$ | $\color{blue}{1}$ | $\color{red}{2}$ | $1$ | 12 |
| $y_{\color{blue}{2},\color{red}{1},1}$ | $\color{blue}{2}$ | $\color{red}{1}$ | $1$ | 11 |
| $y_{\color{blue}{2},\color{red}{1},2}$ | $\color{blue}{2}$ | $\color{red}{1}$ | $2$ | 6 |
| $y_{\color{blue}{3},\color{red}{1},1}$ | $\color{blue}{3}$ | $\color{red}{1}$ | $1$ | 7 |
| $y_{\color{blue}{3},\color{red}{2},2}$ | $\color{blue}{3}$ | $\color{red}{2}$ | $1$ | 14 |

giving the mixed model as
$$\mathbf{y=\color{red}{X}\beta+\color{blue}{Z}u+e}$$
the incident matrix is 
$$
\color{red}{\mathbf{X}=
\begin{bmatrix}
1 & 0\\
0 & 1\\
1 & 0 \\
1 & 0 \\
1 & 0 \\
0 & 1 \\
\end{bmatrix}}
$$
and $\mathbf{\beta}^T=(\beta_1, \beta_2)$
$$
\color{blue}{\mathbf{Z}=
\begin{bmatrix}
1 & 0 & 0\\
1 & 0 & 0\\
0 & 1 & 0\\
0 & 1 & 0\\
0 & 0 & 1 \\
0 & 0 & 1 \\
\end{bmatrix}}
$$
and $\mathbf{u}^T=(u_1, u_2, u_3)$.

```{r, walsh example1}

#walsh example
y=matrix(c(9, 12, 11, 6, 7, 14), 6, 1)
x=matrix(0, 6, 2)
x[,1]=c(1, 0, 1, 1, 1, 0)
x[,2]=c(0, 1, 0, 0, 0, 1)
z=matrix(0, 6, 3)
z[,1]=c(1,1,0,0,0,0)
z[,2]=c(0,0,1,1,0,0)
z[,3]=c(0,0,0,0,1,1)
Se=6
Sa=2
V=Sa*z%*%diag(1, 3, 3)%*%t(z)+Se*diag(1, 6, 6)
VI=solve(V)

bL=solve(t(x)%*%VI%*%x)%*%t(x)%*%VI%*%y
bS=Sa*diag(1, 3, 3)%*%t(z)%*%VI%*%(y-x%*%bL)

#MME
R=diag(Se, 6, 6)
RI=solve(R)
c11=t(x)%*%RI%*%x
c12=t(x)%*%RI%*%z
c21=t(c12)
c22=solve(Sa*diag(1, 3, 3))+t(z)%*%RI%*%z
MME_m=rbind(cbind(c11, c12), cbind(c21, c22))
MME_y=rbind(t(x)%*%RI%*%y, t(z)%*%RI%*%y)

MME_b=solve(MME_m)%*%MME_y
plot(bL, MME_b[1:2,1])
abline(a=0, b=1, col="red")
plot(bS, MME_b[3:5,1])
abline(a=0, b=1, col="red")

```

## Robinson's data
Robinson ("That BLUP is a good thing : the estimation of random effects", Statitical Science, 1991, 6:15-32)

| $\color{red}{Herd}$ (fixed) | $\color{blue}{Sire}$ (random) | Yield |
|:----:|:---:|:----:|
| 1 | A | 110 |
| 1 | D | 100 |
| 2 | B | 110 |
| 2 | D | 100 |
| 2 | D | 100 |
| 3 | C | 110 |
| 3 | C | 110 |
| 3 | D | 100 |
| 3 | D | 100 |

- $$\mathbf{y}=(110,100, 110, 100, 100, 110, 110, 100, 100)^T$$

- The fixed effect is $\mathbf{\beta}=(\beta_1, \beta_2, \beta_3)^T$, and

$$
\color{red}{\mathbf{X}}=
\begin{bmatrix}
1&0&0\\
1&0&0\\
0&1&0\\
0&1&0\\
0&1&0\\
0&0&1\\
0&0&1\\
0&0&1\\
0&0&1\\
\end{bmatrix}
$$

- The sire effect is $\mathbf{\mu}=(s_A, s_B, s_C, s_D)^T$, and
$$
\color{blue}{\mathbf{Z}}=
\begin{bmatrix}
1&0&0&0\\
0&0&0&1\\
0&1&0&0\\
0&0&0&1\\
0&0&0&1\\
0&0&1&0\\
0&0&1&0\\
0&0&0&1\\
0&0&0&1\\
\end{bmatrix}
$$
If we assume $R=I\sigma^2_e$ the identity matrix, and $G=0.1R=0.1I\sigma^2_e$, using MME

$$
\begin{bmatrix}
\mathbf{\color{red}{X}^T\color{red}{X}} & \mathbf{\color{red}{X}^T\color{blue}{Z}} \\
\mathbf{\color{blue}{Z}^T\color{red}{X}} & \mathbf{\color{blue}{Z}^T\color{blue}{Z}}+\frac{\sigma^2_e}{\sigma^2_s}\mathbf{G^{-1}}\\
\end{bmatrix}
\begin{bmatrix}
\mathbf{\color{red}{\hat{\beta}}}\\
\mathbf{\color{blue}{\hat{u}}}\\
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{\color{red}{X}^Ty}\\
\mathbf{\color{blue}{Z}^Ty}\\
\end{bmatrix}
$$

It is realized as 
$$
\begin{bmatrix}
\color{red}{2}&\color{red}{0}&\color{red}{0}&1&0&0&1\\
\color{red}{0}&\color{red}{3}&\color{red}{0}&0&1&0&2\\
\color{red}{0}&\color{red}{0}&\color{red}{4}&0&0&2&2\\
1&0&0&\color{blue}{11}&\color{blue}{0}&\color{blue}{0}&\color{blue}{0}\\
0&1&0&\color{blue}{0}&\color{blue}{11}&\color{blue}{0}&\color{blue}{0}\\
0&0&2&\color{blue}{0}&\color{blue}{0}&\color{blue}{12}&\color{blue}{0}\\
1&1&2&\color{blue}{0}&\color{blue}{0}&\color{blue}{0}&\color{blue}{12}\\
\end{bmatrix}
\begin{bmatrix}
\color{red}{\hat{h}_1}\\
\color{red}{\hat{h}_2}\\
\color{red}{\hat{h}_3}\\
\color{blue}{\hat{s}_A}\\
\color{blue}{\hat{s}_B}\\
\color{blue}{\hat{s}_C}\\
\color{blue}{\hat{s}_D}\\
\end{bmatrix}
=
\begin{bmatrix}
\color{red}{210}\\
\color{red}{310}\\
\color{red}{420}\\
\color{blue}{110}\\
\color{blue}{110}\\
\color{blue}{220}\\
\color{blue}{500}\\
\end{bmatrix}
$$
which has solution

- $\mathbf{\hat{\beta}}=(105.64, 104.28, 105.64)^T$

- $\mathbf{\hat{\mu}}=(0.40, 0.52, 0.76, -1.67)^T$


## Three Blup equivalent models (Animal, Gametic, Red)
### Animal model
$$y_i=\mu+a_i+e_i$$
It is the conventional model used, and $a_i$ is the breeding value of the $i^{th}$ individual, its corresponding LMM can be written as
$$\mathbf{y=\color{red}{X}\beta+\color{blue}{Z}\mu+e}$$
For additive model, $\mathbf{G=A}\sigma^2_A$

It's correponding MME is 

$$
\begin{bmatrix}
\mathbf{\color{red}{X}^T\color{red}{X}} & \mathbf{\color{red}{X}^T\color{blue}{Z}} \\
\mathbf{\color{blue}{Z}^T\color{red}{X}} & \mathbf{\color{blue}{Z}^T\color{blue}{Z}}+\frac{\sigma^2_e}{\sigma^2_s}\mathbf{\color{red}{G}^{-1}}\\
\end{bmatrix}
\begin{bmatrix}
\mathbf{\color{red}{\hat{\beta}}}\\
\mathbf{\color{blue}{\hat{u}}}\\
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{\color{red}{X}^Ty}\\
\mathbf{\color{blue}{Z}^Ty}\\
\end{bmatrix}
$$

### Gematic model

```{r, blup-animal}
Ml=20
Ms=100
N=1000

Xl=apply(matrix(rbinom(Ml*N, 2, 0.5), N, Ml), 2, scale)
Gl=Xl%*%t(Xl)/Ml
Xs=apply(matrix(rbinom(Ms*N, 2, 0.5), N, Ms), 2, scale)
Gs=Xs%*%t(Xs)/Ms

hs=0.5
hsQ=hs/(2*0.5*0.5*Ms)
he=1
bl=rnorm(Ml, 1)
bs=rnorm(Ms, 0, sqrt(hs/(2*0.5*0.5*Ms)))

Fb=Xl%*%bl
Sb=Xs%*%bs
Ev=rnorm(N, 0, sqrt(he))
y=Fb+Sb+Ev

V=Gs*hs+diag(he, N, N)
V_Inv=solve(V)

layout(matrix(1:6, 2, 3, byrow = T))
bl_est=solve(t(Xl)%*%V_Inv%*%Xl)%*%t(Xl)%*%V_Inv%*%y
plot(bl, bl_est, pch=16, cex=1.5, col="green")
abline(a=0, b=1, col="red")
ypre=y-Xl%*%bl_est

bs_est=hs/Ms*t(Xs)%*%V_Inv%*%ypre
plot(bs, bs_est, pch=16, cex=1.5, col="green")
abline(a=0, b=1, col="red")

gs_est=hs/Ms*Xs%*%t(Xs)%*%V_Inv%*%ypre
plot(Sb, gs_est, pch=16, cex=0.5,col="green")
abline(a=0, b=1, col="red")

####MME
Ehe=var(Ev)
c11=t(Xl)%*%Xl
c12=t(Xl)%*%Xs
c21=t(c12)
c22_I=t(Xs)%*%Xs
c22=c22_I+solve(diag(1, Ms, Ms))*he/(hs/Ms)

Y1=t(Xl)%*%y
Y2=t(Xs)%*%y

M1=cbind(c11, c12)
M2=cbind(c21, c22)

MMe_mat=rbind(cbind(c11, c12), cbind(c21, c22))
MMe_y=rbind(Y1, Y2)
bMMe=solve(MMe_mat)%*%MMe_y

plot(bl, bMMe[1:Ml], pch=16, col="blue")
abline(a=0, b=1, col="red")
plot(bs, bMMe[(Ml+1):(Ms+Ml)], col="blue")
abline(a=0, b=1, col="red")
plot(bs_est, bMMe[(Ml+1):(Ms+Ml)], col="blue")
abline(a=0, b=1, col="red")

```

### Gametic model

### Reduced model

## ADD+Dom
Eq 26.21, 26.22, 26.23
```{r, AD}
M=200
N=200
ha=0.5
hd=0.3

frq=rep(0.5, M)
G=matrix(rbinom(M*N, 2, frq), N, M)
Gd=matrix(ifelse(G==1, 1, 0), N, M)
a=rnorm(M)
d=rnorm(M)
BVa=G%*%a
BVd=Gd%*%d

Beta=matrix(c(1, 2), 2, 1)
X=matrix(rbinom(2*N, 2, 0.5), N, 2)
vBVa=var(BVa)[1,1]
vBVd=var(BVd)[1,1]
ve=vBVa+vBVd
y=X%*%Beta+BVa+BVd+rnorm(N, 0, sqrt(ve))

#MME
C11=t(X)%*%X
C12=t(X)
C21=X
C13=t(X)
C31=X
C22=diag(1, M)+1.5*diag(1, M)
C33=diag(1, M)+3*diag(1, M)
MME_1=cbind(C11, C12, C13)
MME_2=cbind(C21, C22, diag(1, M))
MME_3=cbind(C31, diag(1, M), C33)

MME_mat=rbind(MME_1, MME_2, MME_3)
MME_y=matrix(c(t(X)%*%y, y, y), 2*M+2, 1)
MME_b=solve(MME_mat)%*%MME_y
plot(BVa, MME_b[3:(M+2),1])
abline(a=0, b=1)
plot(BVd, MME_b[(M+3):(nrow(MME_b)),1])

##GLMM
V=(diag(vBVa, N)+diag(vBVd, N)+diag(ve, N))
VI=solve(V)
bEst=solve(t(X)%*%VI%*%X)%*%t(X)%*%VI%*%y

uA=vBVa*VI%*%(y-X%*%bEst)
uD=vBVd*VI%*%(y-X%*%bEst)
uD2=(vBVd/vBVa)*uA

plot(uA, MME_b[3:(M+2),1])
abline(a=0, b=1, col="red")
plot(uD, MME_b[(M+3):(nrow(MME_b)),1])
abline(a=0, b=1, col="red")
plot(uD, uD2)
abline(a=0, b=1, col="red")

```


## Simulation setup
```{r simu-setup}
library(Rcpp)
sourceCpp("~/git/Notes/R/RLib/Shotgun.cpp") #https://github.com/gc5k/Notes/blob/master/R/RLib/Shotgun.cpp
###############simulation setup
hl=0.2 #large effect
hs=0.3 #small effect
BLK=20 #number of blocks
BLK_snp=50 #LD block size

M=BLK_snp*BLK #loci
Bloci=seq(10, M, by=BLK) #big effect loci
Ml=length(Bloci) #one large effect in each block
Ms=M-Ml #number of small effect loci

ldInterval=BLK_snp*seq(0, BLK) #ld Tag
frq=runif(M, 0.1, 0.9) #allele frequency
Dp=runif(M-1, 0.8, 1)*sample(c(1,-1), M-1, replace = T) #ld
Dp[BLK_snp*seq(1,BLK-1)]=0 #set break
N=1000 #sample size
Nref=1000 #test

```

## Simulation training and refpop
```{r pop}
####################
#simulating effect
####################
snpEff=array(0, M)
snpEff[-Bloci]=rnorm(Ms, 0, sqrt(hs/Ms))
print(var(snpEff[-Bloci])*Ms)
snpEff[Bloci]=rnorm(Bloci, 0, sqrt(hl/Ml))
print(var(snpEff[Bloci])*Ml)

####################
#simulating genotype
####################
G=GenerateGenoDprimeRcpp(frq, Dp, N)
sG=scale(G) #scale genotype

#simulating additive effect
bv=sG%*%snpEff
ve=var(bv)/(hl+hs)*(1-hl-hs) #scale residual
y=bv+rnorm(N,0, sqrt(ve))

#simulating reference genome for ld block
Gref=GenerateGenoDprimeRcpp(frq, Dp, Nref)
sGref=scale(Gref)


```


## GWAS
```{r gwas}
#########################
#GWAS
#########################
SumStat=matrix(0, M, 4) #summary stats
for(i in 1:nrow(SumStat)) {
  mod=lm(y~sG[,i])
  SumStat[i,]=summary(mod)$coefficient[2,]
}

#####select big loci
BigLoci=Bloci #can be other procedure to pick up big effect loci


```

## Henderson blup

```{r hblup}
##########################################
##Henderson's MME, computational expensive
##########################################
TH_1=Sys.time()
h2_hat=hs+hl #can be estimated wiht other methods. Here we direct plug in the true h2
hs_hat=hs/Ms #h2 for each locus
sGl=sG[,-BigLoci] #matrix for small effect
lGl=sG[,BigLoci] #matrix for large effect

H=hs_hat*sGl%*%t(sGl)+diag(1,N) #Big V matrix
H_inv=solve(H) #inverse it

m1=t(lGl)%*%H_inv%*%lGl
m1_inv=solve(m1)
Bl_henderson=m1_inv%*%t(lGl)%*%H_inv%*%y #generalized estimation for fixed effect

y_res=y-lGl%*%Bl_henderson #residual
Bs_henderson=hs_hat*t(sGl)%*%H_inv%*%(y_res) #blup for random snp effects
TH_2=Sys.time()
print(TH_2-TH_1)

##plot results
layout(matrix(1:6,2,3, byrow = T))
plot(snpEff, SumStat[,1], col="red", pch=16, cex=0.5, xlab="B", ylab="LSE-B")
points(snpEff[BigLoci], SumStat[BigLoci,1], col="blue", pch=2)
abline(a=0, b=1, col="red")

plot(snpEff[-BigLoci], Bs_henderson, col="gold", pch=5, cex=0.5, xlab="Bsmall", ylab="BLUP-Bsmall")
abline(a=0, b=1, col="red")

plot(snpEff[BigLoci], Bl_henderson, col="green", pch=15, xlab="Blarge", ylab="Blarge-blup")
abline(a=0, b=1, col="red")

hist(snpEff[BigLoci], main="B")
hist(SumStat[BigLoci], main="Blup-Bsmall")
hist(Bs_henderson[BigLoci], main="Blup-Blarge")


```

## zBLUP
```{r zBLUP}
########################################
#zxBLUP AJHG v106 p679-693
########################################
LDmatOrg=t(sG)%*%sG/(N-1)

ZH_1=Sys.time()
Ns=ceiling(Nref*1)
subS=sort(sample(Nref, Ns)) #subsampling technique
LDmat=t(sGref[subS,])%*%sGref[subS,]/(Ns-1)

#LDmat=LDmatOrg
ZH_1_1=Sys.time()

hMat=matrix(0, M-length(BigLoci), M-length(BigLoci)) #v matrix
lCnt=0
ldSS_size=matrix(0, BLK, 2)
for(i in 1:BLK) { #block-wise inversion for V using Zhou Xiang's trick
  ld_tag=seq(ldInterval[i]+1, ldInterval[i+1])
  rmLoci=intersect(ld_tag, Bloci)
  if(length(rmLoci)!=0) {
    ld_tag=setdiff(ld_tag, rmLoci)
  }
  ldss=LDmat[ld_tag, ld_tag]
  Ims_sub=diag(1/hs_hat/N, length(ld_tag))
  h=Ims_sub+ldss
  h_Inv=solve(h)
  hMat[(lCnt+1):(lCnt+length(ld_tag)), (lCnt+1):(lCnt+length(ld_tag))]=h_Inv
  ldSS_size[i,]=c(lCnt+1, lCnt+length(ld_tag))
  lCnt=lCnt+length(ld_tag)
}
LDll=LDmat[BigLoci, BigLoci]
LDls=LDmat[BigLoci, -BigLoci]
LDsl=LDmat[-BigLoci, BigLoci]
LDss=LDmat[-BigLoci, -BigLoci]
Zl=SumStat[BigLoci, 3]
Zs=SumStat[-BigLoci, 3]

P1=LDls%*%hMat #there is another trick here to reduce multiplication
C1=(LDll-P1%*%LDsl)
C1_inv=solve(C1)
C2=(Zl-P1%*%Zs)
beta_l_zx=1/sqrt(N)*C1_inv%*%C2

Ims=diag(1, M-length(BigLoci))

ZH_2_1=Sys.time()
P2=matrix(0, nrow(hMat), ncol(hMat))
for(i in 1:nrow(ldSS_size)) {
  l1=ldSS_size[i,1]
  l2=ldSS_size[i,2]
  P2[l1:l2, l1:l2] = LDss[l1:l2, l1:l2] %*% hMat[l1:l2, l1:l2]
}
#P2=LDss%*%hMat
ZH_2_2=Sys.time()

beta_s_zx=hs_hat*(Ims-P2)%*%(sqrt(N)*Zs-N*LDsl%*%beta_l_zx)
ZH_3=Sys.time()

##plot
layout(matrix(1:3, 1,3))
plot(snpEff[BigLoci], beta_l_zx, ylab="Quick Big Effect", xlab="True Big effect")
abline(a=0, b=c(1))
plot(snpEff[-BigLoci], beta_s_zx, ylab="Quick small effect", xlab="True small effect")
abline(a=0, b=c(1))
plot(Bs_henderson, beta_s_zx, xlab="Henderson BLUP", ylab="Quick BLUP")
abline(a=0, b=c(1))

```

## Evalutation

```{r test}
#######################
##prediction accuracy
#######################
SIMU=30
Nt=1000
Gt=GenerateGenoDprimeRcpp(frq, Dp, Nt)
sGt=scale(Gt) #scale genotype

bvt=sGt%*%snpEff
vet=var(bvt)/(hl+hs)*(1-hl-hs) #scale residual

accuracy=matrix(0, SIMU, 3)
for(s in 1:SIMU) {
  yt=bvt+rnorm(Nt,0, sqrt(vet))

  yPre_zx=sGt[,BigLoci]%*%beta_l_zx+sGt[,-BigLoci]%*%beta_s_zx
  yPre_h=sGt[,BigLoci]%*%Bl_henderson+sGt[,-BigLoci]%*%Bs_henderson
  yPre_lse=sGt[,BigLoci]%*%SumStat[BigLoci,1]+sGt[,-BigLoci]%*%SumStat[-BigLoci,1]
  accuracy[s,1]=cor(yt, yPre_zx)
  accuracy[s,2]=cor(yt, yPre_h)
  accuracy[s,3]=cor(yt, yPre_lse)
}
barplot(colMeans(accuracy))


```

